# -*- coding: utf-8 -*-
"""AUTO COMPLETE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19hayMWwt_zvaCz16lhNPvRU1fQsMcs9S

# **PART A**

1. Load and Print Data
"""

def load_and_print_data(file_path):
    with open(file_path, 'r') as file:
        data = file.read()
    print(data[:500])
    return data

data = load_and_print_data("/content/sample_data/twitter.txt")

"""2. Split to Sentences"""

import re

def split_to_sentences(data):
    sentences = re.split(r'[.!?]+', data)
    return sentences

sentences = split_to_sentences(data)
print(sentences[:100])

"""3. Tokenize Sentences"""

import nltk
nltk.download('punkt')

def tokenize_sentences(sentences):
    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
    return tokenized_sentences

tokenized_sentences = tokenize_sentences(sentences)
print(tokenized_sentences[:1000])

"""4. Tokenize Data"""

def tokenize_data(data):
    sentences = split_to_sentences(data)
    tokenized_sentences = tokenize_sentences(sentences)
    return tokenized_sentences

tokenized_data = tokenize_data(data)
print(tokenized_data[:1000])

"""5. Test / Train Split

"""

from sklearn.model_selection import train_test_split

def split_data(tokenized_data):
    X_train, X_test, y_train, y_test = train_test_split(tokenized_data, tokenized_data, test_size=0.2, random_state=42)
    return X_train, X_test

X_train, X_test = split_data(tokenized_data)
print("Training Data:", X_train[:100])
print("Test Data:", X_test[:100])

"""6. Count Words

"""

from collections import Counter

def count_words(tokenized_sentences):
    word_counts = Counter()
    for sentence in tokenized_sentences:
        word_counts.update(sentence)
    return word_counts

word_counts = count_words(tokenized_data)
print(word_counts)

"""7. Words with N Frequency

"""

def get_closed_vocabulary(tokenized_sentences, threshold):
    word_counts = count_words(tokenized_sentences)
    closed_vocabulary = set(word for word, count in word_counts.items() if count >= threshold)
    return closed_vocabulary

closed_vocabulary = get_closed_vocabulary(tokenized_data, 2)
print(closed_vocabulary)

"""8. Replace OOV with UNK"""

def replace_oov_with_unk(tokenized_sentences, vocabulary, unknown_token='<unk>'):
    replaced_sentences = []
    for sentence in tokenized_sentences:
        replaced_sentence = [word if word in vocabulary else unknown_token for word in sentence]
        replaced_sentences.append(replaced_sentence)
    return replaced_sentences

replaced_tokenized_data = replace_oov_with_unk(tokenized_data, closed_vocabulary)
print(replaced_tokenized_data[:1000])

"""9. Pre-process Data"""

def preprocess_data(train_data, test_data, threshold):
    closed_vocabulary = get_closed_vocabulary(train_data + test_data, threshold)
    train_data_replaced = replace_oov_with_unk(train_data, closed_vocabulary)
    test_data_replaced = replace_oov_with_unk(test_data, closed_vocabulary)
    return train_data_replaced, test_data_replaced, closed_vocabulary

train_data_replaced, test_data_replaced, vocabulary = preprocess_data(X_train, X_test, 2)
print("Preprocessed Training Data:", train_data_replaced[:1000])
print("Preprocessed Test Data:", test_data_replaced[:1000])
print("Vocabulary:", vocabulary)

"""# **PART B**

10. Count N-Grams
"""

from collections import defaultdict
import itertools

def add_start_end_tokens(sentences, n):
    start_token = "<s>"
    end_token = "<e>"
    padded_sentences = [
        [start_token] * (n-1) + sentence + [end_token] for sentence in sentences
    ]
    return padded_sentences

def count_ngrams(sentences, n):
    ngrams = defaultdict(int)
    for sentence in sentences:
        for i in range(len(sentence) - n + 1):
            ngram = tuple(sentence[i:i+n])
            ngrams[ngram] += 1
    return ngrams

n = 2
padded_tokenized_sentences = add_start_end_tokens(tokenized_sentences, n)
bigrams = count_ngrams(padded_tokenized_sentences, n)
print(list(bigrams.items())[:10])

"""11. Estimate Probability of Single Word"""

def estimate_probability(word, prev_ngram, ngram_counts, nplus1gram_counts, vocab_size, k=1):
    ngram = tuple(prev_ngram)
    nplus1gram = tuple(prev_ngram + [word])
    ngram_count = ngram_counts[ngram]
    nplus1gram_count = nplus1gram_counts[nplus1gram]
    probability = (nplus1gram_count + k) / (ngram_count + k * vocab_size)
    return probability

word = 'new'
prev_ngram = ['<s>']
vocab_size = len(vocabulary)
nplus1grams = count_ngrams(padded_tokenized_sentences, n+1)
probability = estimate_probability(word, prev_ngram, bigrams, nplus1grams, vocab_size)
print("Probability:", probability)

"""12. Estimate Probability of All Words"""

def estimate_all_probabilities(prev_ngram, ngram_counts, nplus1gram_counts, vocab_size, k=1):
    probabilities = {}
    for word in vocabulary:
        probabilities[word] = estimate_probability(word, prev_ngram, ngram_counts, nplus1gram_counts, vocab_size, k)
    return probabilities

all_probabilities = estimate_all_probabilities(prev_ngram, bigrams, nplus1grams, vocab_size)
print(list(all_probabilities.items())[:10])

"""13. Make Count Matrix"""

def make_count_matrix(nplus1gram_counts, vocab):
    vocab_size = len(vocab)
    vocab_list = list(vocab)
    index = {word: i for i, word in enumerate(vocab_list)}
    count_matrix = np.zeros((vocab_size, vocab_size), dtype=int)

    for nplus1gram, count in nplus1gram_counts.items():
        prev_ngram = tuple(nplus1gram[:-1])
        word = nplus1gram[-1]
        if prev_ngram[-1] in index and word in index:
            row = index[prev_ngram[-1]]
            col = index[word]
            count_matrix[row][col] = count

    print("Count Matrix:")
    print(count_matrix)

    return count_matrix

count_matrix = make_count_matrix(nplus1grams, vocabulary)

"""14. Make Probability Matrix"""

import numpy as np

def make_probability_matrix(n_plus_1_gram_counts, vocabulary, k=1):
    vocab_list = list(vocabulary)
    vocab_size = len(vocab_list)
    count_matrix = np.zeros((vocab_size, vocab_size))

    for ngram, count in n_plus_1_gram_counts.items():
        ngram_split = ngram.split()
        if len(ngram_split) == 2:
            try:
                i = vocab_list.index(ngram_split[0])
                j = vocab_list.index(ngram_split[1])
                count_matrix[i, j] = count
            except ValueError:
                pass
    smoothed_count_matrix = count_matrix + k
    row_sums = smoothed_count_matrix.sum(axis=1, keepdims=True)

    with np.errstate(divide='ignore', invalid='ignore'):
        probability_matrix = np.divide(smoothed_count_matrix, row_sums, where=row_sums != 0)

    return probability_matrix
print("Probability Matrix:\n", probability_matrix)

"""# **PART C**

15. Calculate Perplexity
"""

import numpy as np
from collections import Counter

def calculate_perplexity(sentence, ngram_counts, n_plus_1_gram_counts, vocabulary_size, k=0.01):
    n = len(next(iter(ngram_counts)))
    sentence = ["<s>"] * (n-1) + sentence + ["</s>"]
    N = len(sentence)
    log_prob = 0.0
    for i in range(n-1, N):
        previous_ngram = sentence[i-n+1:i]
        word = sentence[i]
        probability = estimate_probability(word, previous_ngram, ngram_counts, n_plus_1_gram_counts, vocabulary_size, k)
        log_prob += np.log(probability)
    perplexity = np.exp(-log_prob / (N - (n-1)))
    return perplexity

sample_sentence = ["I", "am", "doing", "project"]

unigram = count_ngrams(sample_sentence, 1)
bigram = count_ngrams(sample_sentence, 2)

perplexity = calculate_perplexity(sample_sentence, unigram, bigram, len(sample_sentence), k=0.01)
print("Perplexity:", perplexity)

"""# **PART D**

16. Suggest a Word
"""

def suggest_word(prev_tokens, ngram_counts, nplus1gram_counts, vocab_size, k=1, start_with=None):
    probabilities = estimate_all_probabilities(prev_tokens, ngram_counts, nplus1gram_counts, vocab_size, k)

    if start_with:
        probabilities = {word: prob for word, prob in probabilities.items() if word.startswith(start_with)}

    suggestion = max(probabilities, key=probabilities.get)
    max_probability = probabilities[suggestion]

    return suggestion, max_probability

suggestion, max_probability = suggest_word(prev_tokens, bigrams, nplus1grams, vocab_size, k=1, start_with=None)
print(f"Suggestion: {suggestion}, Max Probability: {max_probability}")

"""17. Get Suggestions"""

def get_all_suggestions(prev_tokens, ngram_counts, nplus1gram_counts, vocabulary, k=1, start_with=None):
    probabilities = estimate_all_probabilities(prev_tokens, ngram_counts, nplus1gram_counts, len(vocabulary), k)

    if start_with:
        probabilities = {word: prob for word, prob in probabilities.items() if word.startswith(start_with)}

    sorted_suggestions = sorted(probabilities.items(), key=lambda item: item[1], reverse=True)

    return sorted_suggestions

all_suggestions = get_all_suggestions(prev_tokens, bigrams, nplus1grams, vocabulary, k=1, start_with=None)
print("All Suggestions:")
for word, prob in all_suggestions[:10]:
    print(f"Word: {word}, Probability: {prob}")